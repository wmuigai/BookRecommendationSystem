{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e38af9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-surprise in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ratings.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy\n\u001b[0;32m      7\u001b[0m reader \u001b[38;5;241m=\u001b[39m Reader(line_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser item rating\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mload_from_file(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratings.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, reader\u001b[38;5;241m=\u001b[39mreader)\n\u001b[0;32m     11\u001b[0m trainset, testset \u001b[38;5;241m=\u001b[39m train_test_split(data, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m)\n\u001b[0;32m     14\u001b[0m algo \u001b[38;5;241m=\u001b[39m KNNBasic()\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\surprise\\dataset.py:123\u001b[0m, in \u001b[0;36mDataset.load_from_file\u001b[1;34m(cls, file_path, reader)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, file_path, reader):\n\u001b[0;32m    109\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a dataset from a (custom) file.\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Use this if you want to use a custom dataset and all of the ratings are\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m            the file.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatasetAutoFolds(ratings_file\u001b[38;5;241m=\u001b[39mfile_path, reader\u001b[38;5;241m=\u001b[39mreader)\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\surprise\\dataset.py:259\u001b[0m, in \u001b[0;36mDatasetAutoFolds.__init__\u001b[1;34m(self, ratings_file, reader, df)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ratings_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mratings_file \u001b[38;5;241m=\u001b[39m ratings_file\n\u001b[1;32m--> 259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_ratings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_ratings(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mratings_file)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m df\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\surprise\\dataset.py:173\u001b[0m, in \u001b[0;36mDataset.read_ratings\u001b[1;34m(self, file_name)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_ratings\u001b[39m(\u001b[38;5;28mself\u001b[39m, file_name):\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of ratings (user, item, rating, timestamp) read from\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m    file_name\"\"\"\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(file_name)) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    174\u001b[0m         raw_ratings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    175\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader\u001b[38;5;241m.\u001b[39mparse_line(line)\n\u001b[0;32m    176\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mislice(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader\u001b[38;5;241m.\u001b[39mskip_lines, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    177\u001b[0m         ]\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m raw_ratings\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ratings.csv'"
     ]
    }
   ],
   "source": [
    "%pip install scikit-surprise\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "\n",
    "reader = Reader(line_format='user item rating', sep=',')\n",
    "data = Dataset.load_from_file('ratings.csv', reader=reader)\n",
    "\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "\n",
    "algo = KNNBasic()\n",
    "    \n",
    "algo.fit(trainset)\n",
    "\n",
    "\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "\n",
    "accuracy.rmse(predictions)\n",
    "accuracy.mae(predictions)\n",
    "\n",
    "\n",
    "user_id = 'user1'\n",
    "items_to_recommend = []\n",
    "already_rated = set([item[0] for item in trainset.ur[trainset.to_inner_uid(user_id)]])\n",
    "for item_id in trainset.all_items():\n",
    "    if item_id not in already_rated:\n",
    "        items_to_recommend.append((item_id, algo.predict(user_id, item_id).est))\n",
    "\n",
    "\n",
    "sorted_recommendations = sorted(items_to_recommend, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "print(\"Top Recommendations:\")\n",
    "for item_id, rating in sorted_recommendations[:10]:\n",
    "    print(f\"Item ID: {item_id}, Predicted Rating: {rating}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b093a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_curve\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046880a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# read data \u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv'"
     ]
    }
   ],
   "source": [
    "# read data \n",
    "df = pd.read_csv(\"/kaggle/input/heart-attack-analysis-prediction-dataset/heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db079658",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '?'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9264\\1377563204.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Train the model (Random Forest Classifier)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Make predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1147\u001b[0m                 skip_parameter_validation=(\n\u001b[0;32m   1148\u001b[0m                     \u001b[0mprefer_skip_nested_validation\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mglobal_skip_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m                 )\n\u001b[0;32m   1150\u001b[0m             ):\n\u001b[1;32m-> 1151\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    344\u001b[0m         \"\"\"\n\u001b[0;32m    345\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m         X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    349\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         )\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    617\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 621\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    622\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ensure_2d\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1143\u001b[0m         raise ValueError(\n\u001b[0;32m   1144\u001b[0m             \u001b[1;34mf\"{estimator_name} requires y to be passed, but the target y is None\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1145\u001b[0m         )\n\u001b[0;32m   1146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m     X = check_array(\n\u001b[0m\u001b[0;32m   1148\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    914\u001b[0m                         )\n\u001b[0;32m    915\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m                 raise ValueError(\n\u001b[0;32m    920\u001b[0m                     \u001b[1;34m\"Complex data not supported\\n{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m                 ) from complex_warning\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;31m# Use NumPy API to support order\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m             \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;31m# container that is consistent with the input's namespace.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1996\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDTypeLike\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         if (\n\u001b[0;32m   2000\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '?'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "column_names = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"]\n",
    "data = pd.read_csv(url, names=column_names)\n",
    "\n",
    "# Drop missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model (Random Forest Classifier)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b6c7fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 119) (3285362298.py, line 119)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 119\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"88151edb5313448…\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 119)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "{\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 0,\n",
    "  \"metadata\": {\n",
    "    \"colab\": {\n",
    "      \"name\": \"Recommendation_System.ipynb\",\n",
    "      \"provenance\": [],\n",
    "      \"collapsed_sections\": []\n",
    "    },\n",
    "    \"kernelspec\": {\n",
    "      \"name\": \"python3\",\n",
    "      \"display_name\": \"Python 3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"name\": \"python\"\n",
    "    },\n",
    "    \"accelerator\": \"GPU\",\n",
    "    \"widgets\": {\n",
    "      \"application/vnd.jupyter.widget-state+json\": {\n",
    "        \"3c1765cabeaf4d75a130311ddc4591a1\": {\n",
    "          \"model_module\": \"@jupyter-widgets/controls\",\n",
    "          \"model_name\": \"HBoxModel\",\n",
    "          \"state\": {\n",
    "            \"_view_name\": \"HBoxView\",\n",
    "            \"_dom_classes\": [],\n",
    "            \"_model_name\": \"HBoxModel\",\n",
    "            \"_view_module\": \"@jupyter-widgets/controls\",\n",
    "            \"_model_module_version\": \"1.5.0\",\n",
    "            \"_view_count\": null,\n",
    "            \"_view_module_version\": \"1.5.0\",\n",
    "            \"box_style\": \"\",\n",
    "            \"layout\": \"IPY_MODEL_6db170b52f5a4f72afd083a3a9c157aa\",\n",
    "            \"_model_module\": \"@jupyter-widgets/controls\",\n",
    "            \"children\": [\n",
    "              \"IPY_MODEL_69073fd458234bba829af9731b4fce1c\",\n",
    "              \"IPY_MODEL_88151edb53134487b95f6bcbcd7fd2b4\"\n",
    "            ]\n",
    "          }\n",
    "        },\n",
    "        \"6db170b52f5a4f72afd083a3a9c157aa\": {\n",
    "          \"model_module\": \"@jupyter-widgets/base\",\n",
    "          \"model_name\": \"LayoutModel\",\n",
    "          \"state\": {\n",
    "            \"_view_name\": \"LayoutView\",\n",
    "            \"grid_template_rows\": null,\n",
    "            \"right\": null,\n",
    "            \"justify_content\": null,\n",
    "            \"_view_module\": \"@jupyter-widgets/base\",\n",
    "            \"overflow\": null,\n",
    "            \"_model_module_version\": \"1.2.0\",\n",
    "            \"_view_count\": null,\n",
    "            \"flex_flow\": null,\n",
    "            \"width\": null,\n",
    "            \"min_width\": null,\n",
    "            \"border\": null,\n",
    "            \"align_items\": null,\n",
    "            \"bottom\": null,\n",
    "            \"_model_module\": \"@jupyter-widgets/base\",\n",
    "            \"top\": null,\n",
    "            \"grid_column\": null,\n",
    "            \"overflow_y\": null,\n",
    "            \"overflow_x\": null,\n",
    "            \"grid_auto_flow\": null,\n",
    "            \"grid_area\": null,\n",
    "            \"grid_template_columns\": null,\n",
    "            \"flex\": null,\n",
    "            \"_model_name\": \"LayoutModel\",\n",
    "            \"justify_items\": null,\n",
    "            \"grid_row\": null,\n",
    "            \"max_height\": null,\n",
    "            \"align_content\": null,\n",
    "            \"visibility\": null,\n",
    "            \"align_self\": null,\n",
    "            \"height\": null,\n",
    "            \"min_height\": null,\n",
    "            \"padding\": null,\n",
    "            \"grid_auto_rows\": null,\n",
    "            \"grid_gap\": null,\n",
    "            \"max_width\": null,\n",
    "            \"order\": null,\n",
    "            \"_view_module_version\": \"1.2.0\",\n",
    "            \"grid_template_areas\": null,\n",
    "            \"object_position\": null,\n",
    "            \"object_fit\": null,\n",
    "            \"grid_auto_columns\": null,\n",
    "            \"margin\": null,\n",
    "            \"display\": null,\n",
    "            \"left\": null\n",
    "          }\n",
    "        },\n",
    "        \"69073fd458234bba829af9731b4fce1c\": {\n",
    "          \"model_module\": \"@jupyter-widgets/controls\",\n",
    "          \"model_name\": \"FloatProgressModel\",\n",
    "          \"state\": {\n",
    "            \"_view_name\": \"ProgressView\",\n",
    "            \"style\": \"IPY_MODEL_9b185906b1dc465e93b9f058a1fec47a\",\n",
    "            \"_dom_classes\": [],\n",
    "            \"description\": \"100%\",\n",
    "            \"_model_name\": \"FloatProgressModel\",\n",
    "            \"bar_style\": \"success\",\n",
    "            \"max\": 128,\n",
    "            \"_view_module\": \"@jupyter-widgets/controls\",\n",
    "            \"_model_module_version\": \"1.5.0\",\n",
    "            \"value\": 128,\n",
    "            \"_view_count\": null,\n",
    "            \"_view_module_version\": \"1.5.0\",\n",
    "            \"orientation\": \"horizontal\",\n",
    "            \"min\": 0,\n",
    "            \"description_tooltip\": null,\n",
    "            \"_model_module\": \"@jupyter-widgets/controls\",\n",
    "            \"layout\": \"IPY_MODEL_20b4738815cd467dba2946031d6d9027\"\n",
    "          }\n",
    "        },\n",
    "        \"88151edb5313448…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267131e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\hp\\Desktop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory:\", current_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493ef6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, Reader, KNNBasic\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Read the dataset from 'ratings.csv'\n",
    "reader = Reader(line_format='user item rating', sep=',')\n",
    "data = Dataset.load_from_file('ratings.csv', reader=reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Initialize KNNBasic with appropriate parameters\n",
    "sim_options = {'name': 'cosine', 'user_based': True}  # Set user-based similarity\n",
    "algo = KNNBasic(k=40, sim_options=sim_options)\n",
    "\n",
    "# Fit the algorithm to the training data\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))\n",
    "print(\"MAE:\", accuracy.mae(predictions))\n",
    "\n",
    "# Generate recommendations for user 'user1'\n",
    "user_id = 'user1'\n",
    "items_to_recommend = []\n",
    "already_rated = set([item[0] for item in trainset.ur[trainset.to_inner_uid(user_id)]])\n",
    "for item_id in trainset.all_items():\n",
    "    if item_id not in already_rated:\n",
    "        items_to_recommend.append((item_id, algo.predict(user_id, item_id).est))\n",
    "\n",
    "# Sort recommendations by predicted rating\n",
    "sorted_recommendations = sorted(items_to_recommend, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top Recommendations:\")\n",
    "for item_id, rating in sorted_recommendations[:10]:\n",
    "    print(f\"Item ID: {item_id}, Predicted Rating: {rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d615881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "  Using cached scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.1)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py): started\n",
      "  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for scikit-surprise\n",
      "Failed to build scikit-surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [101 lines of output]\n",
      "  C:\\Users\\hp\\AppData\\Local\\Temp\\pip-install-1rlf8kmv\\scikit-surprise_a3d13b50465346aaa5233596c458ea67\\setup.py:65: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Requirements should be satisfied by a PEP 517 installer.\n",
      "          If you are using pip, you can try `pip install --use-pep517`.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    dist.Distribution().fetch_build_eggs([\"numpy>=1.17.3\"])\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\accuracy.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\dataset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\dump.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\reader.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\trainset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\utils.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\__main__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  running egg_info\n",
      "  writing scikit_surprise.egg-info\\PKG-INFO\n",
      "  writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "  writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "  writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "  writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "  reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\setuptools\\command\\build_py.py:201: _Warning: Package 'surprise.prediction_algorithms' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'surprise.prediction_algorithms' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'surprise.prediction_algorithms' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'surprise.prediction_algorithms' to be distributed and are\n",
      "          already explicitly excluding 'surprise.prediction_algorithms' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  copying surprise\\similarities.c -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\similarities.pyx -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  running build_ext\n",
      "  building 'surprise.similarities' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for scikit-surprise\n",
      "ERROR: Could not build wheels for scikit-surprise, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac37c86",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, Reader, KNNBasic\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Read the dataset from 'ratings.csv'\n",
    "reader = Reader(line_format='user item rating', sep=',')\n",
    "data = Dataset.load_from_file('ratings.csv', reader=reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Initialize KNNBasic with appropriate parameters\n",
    "sim_options = {'name': 'cosine', 'user_based': True}  # Set user-based similarity\n",
    "algo = KNNBasic(k=40, sim_options=sim_options)\n",
    "\n",
    "# Fit the algorithm to the training data\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))\n",
    "print(\"MAE:\", accuracy.mae(predictions))\n",
    "\n",
    "# Generate recommendations for user 'user1'\n",
    "user_id = 'user1'\n",
    "items_to_recommend = []\n",
    "already_rated = set([item[0] for item in trainset.ur[trainset.to_inner_uid(user_id)]])\n",
    "for item_id in trainset.all_items():\n",
    "    if item_id not in already_rated:\n",
    "        items_to_recommend.append((item_id, algo.predict(user_id, item_id).est))\n",
    "\n",
    "# Sort recommendations by predicted rating\n",
    "sorted_recommendations = sorted(items_to_recommend, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top Recommendations:\")\n",
    "for item_id, rating in sorted_recommendations[:10]:\n",
    "    print(f\"Item ID: {item_id}, Predicted Rating: {rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83524604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [101 lines of output]\n",
      "  C:\\Users\\hp\\AppData\\Local\\Temp\\pip-install-53anxrhz\\scikit-surprise_e0f10d4ae32c420fb3fb2ecbd4958202\\setup.py:65: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Requirements should be satisfied by a PEP 517 installer.\n",
      "          If you are using pip, you can try `pip install --use-pep517`.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    dist.Distribution().fetch_build_eggs([\"numpy>=1.17.3\"])\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\accuracy.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\dataset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\dump.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\reader.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\trainset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\utils.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\__main__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  running egg_info\n",
      "  writing scikit_surprise.egg-info\\PKG-INFO\n",
      "  writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "  writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "  writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "  writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "  reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\setuptools\\command\\build_py.py:201: _Warning: Package 'surprise.prediction_algorithms' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'surprise.prediction_algorithms' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'surprise.prediction_algorithms' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'surprise.prediction_algorithms' to be distributed and are\n",
      "          already explicitly excluding 'surprise.prediction_algorithms' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  copying surprise\\similarities.c -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\similarities.pyx -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  running build_ext\n",
      "  building 'surprise.similarities' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for scikit-surprise\n",
      "ERROR: Could not build wheels for scikit-surprise, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.1)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py): started\n",
      "  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for scikit-surprise\n",
      "Failed to build scikit-surprise\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7b1331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "  Using cached scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.1)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py): started\n",
      "  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for scikit-surprise\n",
      "Failed to build scikit-surprise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [101 lines of output]\n",
      "  C:\\Users\\hp\\AppData\\Local\\Temp\\pip-install-579hxz0f\\scikit-surprise_08baefa47859491586acc8d4bf190303\\setup.py:65: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Requirements should be satisfied by a PEP 517 installer.\n",
      "          If you are using pip, you can try `pip install --use-pep517`.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    dist.Distribution().fetch_build_eggs([\"numpy>=1.17.3\"])\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\accuracy.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\dataset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\dump.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\reader.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\trainset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\utils.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\__main__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  running egg_info\n",
      "  writing scikit_surprise.egg-info\\PKG-INFO\n",
      "  writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "  writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "  writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "  writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "  reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\setuptools\\command\\build_py.py:201: _Warning: Package 'surprise.prediction_algorithms' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'surprise.prediction_algorithms' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'surprise.prediction_algorithms' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'surprise.prediction_algorithms' to be distributed and are\n",
      "          already explicitly excluding 'surprise.prediction_algorithms' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  copying surprise\\similarities.c -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\similarities.pyx -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  running build_ext\n",
      "  building 'surprise.similarities' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for scikit-surprise\n",
      "ERROR: Could not build wheels for scikit-surprise, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'surprise'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install scikit-surprise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset, Reader, KNNBasic\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msurprise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'surprise'"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n",
    "\n",
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Read the dataset from 'ratings.csv'\n",
    "reader = Reader(line_format='user item rating', sep=',')\n",
    "data = Dataset.load_from_file('ratings.csv', reader=reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Initialize KNNBasic with appropriate parameters\n",
    "sim_options = {'name': 'cosine', 'user_based': True}  # Set user-based similarity\n",
    "algo = KNNBasic(k=40, sim_options=sim_options)\n",
    "\n",
    "# Fit the algorithm to the training data\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))\n",
    "print(\"MAE:\", accuracy.mae(predictions))\n",
    "\n",
    "# Generate recommendations for user 'user1'\n",
    "user_id = 'user1'\n",
    "items_to_recommend = []\n",
    "already_rated = set([item[0] for item in trainset.ur[trainset.to_inner_uid(user_id)]])\n",
    "for item_id in trainset.all_items():\n",
    "    if item_id not in already_rated:\n",
    "        items_to_recommend.append((item_id, algo.predict(user_id, item_id).est))\n",
    "\n",
    "# Sort recommendations by predicted rating\n",
    "sorted_recommendations = sorted(items_to_recommend, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top Recommendations:\")\n",
    "for item_id, rating in sorted_recommendations[:10]:\n",
    "    print(f\"Item ID: {item_id}, Predicted Rating: {rating:.2f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2576bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [101 lines of output]\n",
      "  C:\\Users\\hp\\AppData\\Local\\Temp\\pip-install-8hn28uys\\scikit-surprise_205cd42db0fc40b4a6345efcb9bf9d66\\setup.py:65: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Requirements should be satisfied by a PEP 517 installer.\n",
      "          If you are using pip, you can try `pip install --use-pep517`.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    dist.Distribution().fetch_build_eggs([\"numpy>=1.17.3\"])\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-311\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\accuracy.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\dataset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\dump.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\reader.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\trainset.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\utils.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\__main__.py -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\model_selection\n",
      "  creating build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  running egg_info\n",
      "  writing scikit_surprise.egg-info\\PKG-INFO\n",
      "  writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "  writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "  writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "  writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "  reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\hp\\anaconda3\\Lib\\site-packages\\setuptools\\command\\build_py.py:201: _Warning: Package 'surprise.prediction_algorithms' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'surprise.prediction_algorithms' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'surprise.prediction_algorithms' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'surprise.prediction_algorithms' to be distributed and are\n",
      "          already explicitly excluding 'surprise.prediction_algorithms' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  copying surprise\\similarities.c -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\similarities.pyx -> build\\lib.win-amd64-cpython-311\\surprise\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-cpython-311\\surprise\\prediction_algorithms\n",
      "  running build_ext\n",
      "  building 'surprise.similarities' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for scikit-surprise\n",
      "ERROR: Could not build wheels for scikit-surprise, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.1)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py): started\n",
      "  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for scikit-surprise\n",
      "Failed to build scikit-surprise\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa782217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-surprise in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_1404\\2112658178.py:10: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv('ratings.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ISBN                                         Book-Title  \\\n",
      "0  0195153448                                Classical Mythology   \n",
      "1  0002005018                                       Clara Callan   \n",
      "2  0060973129                               Decision in Normandy   \n",
      "3  0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
      "4  0393045218                             The Mummies of Urumchi   \n",
      "\n",
      "            Book-Author Year-Of-Publication                   Publisher  \\\n",
      "0    Mark P. O. Morford                2002     Oxford University Press   \n",
      "1  Richard Bruce Wright                2001       HarperFlamingo Canada   \n",
      "2          Carlo D'Este                1991             HarperPerennial   \n",
      "3      Gina Bari Kolata                1999        Farrar Straus Giroux   \n",
      "4       E. J. W. Barber                1999  W. W. Norton &amp; Company   \n",
      "\n",
      "                                         Image-URL-S  \\\n",
      "0  http://images.amazon.com/images/P/0195153448.0...   \n",
      "1  http://images.amazon.com/images/P/0002005018.0...   \n",
      "2  http://images.amazon.com/images/P/0060973129.0...   \n",
      "3  http://images.amazon.com/images/P/0374157065.0...   \n",
      "4  http://images.amazon.com/images/P/0393045218.0...   \n",
      "\n",
      "                                         Image-URL-M  \\\n",
      "0  http://images.amazon.com/images/P/0195153448.0...   \n",
      "1  http://images.amazon.com/images/P/0002005018.0...   \n",
      "2  http://images.amazon.com/images/P/0060973129.0...   \n",
      "3  http://images.amazon.com/images/P/0374157065.0...   \n",
      "4  http://images.amazon.com/images/P/0393045218.0...   \n",
      "\n",
      "                                         Image-URL-L  \n",
      "0  http://images.amazon.com/images/P/0195153448.0...  \n",
      "1  http://images.amazon.com/images/P/0002005018.0...  \n",
      "2  http://images.amazon.com/images/P/0060973129.0...  \n",
      "3  http://images.amazon.com/images/P/0374157065.0...  \n",
      "4  http://images.amazon.com/images/P/0393045218.0...  \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'raw_ratings'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1404\\2112658178.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ratings.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Split the data into training and test sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# Initialize KNNBasic with appropriate parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0msim_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'cosine'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'user_based'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m  \u001b[1;31m# Set user-based similarity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\surprise\\model_selection\\split.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(data, test_size, train_size, random_state, shuffle)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     )\n\u001b[1;32m--> 355\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\surprise\\model_selection\\split.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[0mtuple\u001b[0m \u001b[0mof\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \"\"\"\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         test_size, train_size = self.validate_train_test_sizes(\n\u001b[1;32m--> 289\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_ratings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m         )\n\u001b[0;32m    291\u001b[0m         \u001b[0mrng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_rng\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5985\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5986\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'raw_ratings'"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n",
    "\n",
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Read the dataset from 'ratings.csv'\n",
    "reader = Reader(line_format='user item rating', sep=',')\n",
    "data = Dataset.load_from_df(df[['user', 'item', 'rating']], reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Initialize KNNBasic with appropriate parameters\n",
    "sim_options = {'name': 'cosine', 'user_based': True}  # Set user-based similarity\n",
    "algo = KNNBasic(k=40, sim_options=sim_options)\n",
    "\n",
    "# Fit the algorithm to the training data\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))\n",
    "print(\"MAE:\", accuracy.mae(predictions))\n",
    "\n",
    "# Generate recommendations for user 'user1'\n",
    "user_id = 'user1'\n",
    "items_to_recommend = []\n",
    "already_rated = set([item[0] for item in trainset.ur[trainset.to_inner_uid(user_id)]])\n",
    "for item_id in trainset.all_items():\n",
    "    if item_id not in already_rated:\n",
    "        items_to_recommend.append((item_id, algo.predict(user_id, item_id).est))\n",
    "\n",
    "# Sort recommendations by predicted rating\n",
    "sorted_recommendations = sorted(items_to_recommend, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top Recommendations:\")\n",
    "for item_id, rating in sorted_recommendations[:10]:\n",
    "    print(f\"Item ID: {item_id}, Predicted Rating: {rating:.2f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f19981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\hp\\Desktop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(\"Current working directory:\", cwd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f7b7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-surprise in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_1404\\4169962197.py:9: DtypeWarning: Columns (0,1,2,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('ratings.csv')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['user', 'item', 'rating'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m reader \u001b[38;5;241m=\u001b[39m Reader(rating_scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load the dataset from the DataFrame using the appropriate columns\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m data \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mload_from_df(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mitem\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m]], reader)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Split the data into training and test sets\u001b[39;00m\n\u001b[0;32m     18\u001b[0m trainset, testset \u001b[38;5;241m=\u001b[39m train_test_split(data, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3766\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3767\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3769\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5879\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5881\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   5937\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 5938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5940\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   5941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['user', 'item', 'rating'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n",
    "\n",
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Read the dataset from 'ratings.csv' into a DataFrame\n",
    "df = pd.read_csv('ratings.csv')\n",
    "\n",
    "# Define the Reader object\n",
    "reader = Reader(rating_scale=(0, 10))\n",
    "\n",
    "# Load the dataset from the DataFrame using the appropriate columns\n",
    "data = Dataset.load_from_df(df[['user', 'item', 'rating']], reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Initialize KNNBasic with appropriate parameters\n",
    "sim_options = {'name': 'cosine', 'user_based': True}  # Set user-based similarity\n",
    "algo = KNNBasic(k=40, sim_options=sim_options)\n",
    "\n",
    "# Fit the algorithm to the training data\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))\n",
    "print(\"MAE:\", accuracy.mae(predictions))\n",
    "\n",
    "# Generate recommendations for user 'user1'\n",
    "user_id = 'user1'\n",
    "items_to_recommend = []\n",
    "already_rated = set([item[0] for item in trainset.ur[trainset.to_inner_uid(user_id)]])\n",
    "for item_id in trainset.all_items():\n",
    "    if item_id not in already_rated:\n",
    "        items_to_recommend.append((item_id, algo.predict(user_id, item_id).est))\n",
    "\n",
    "# Sort recommendations by predicted rating\n",
    "sorted_recommendations = sorted(items_to_recommend, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top Recommendations:\")\n",
    "for item_id, rating in sorted_recommendations[:10]:\n",
    "    print(f\"Item ID: {item_id}, Predicted Rating: {rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9745ed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-surprise in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_1404\\2314205753.py:9: DtypeWarning: Columns (0,1,2,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('ratings.csv')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['user_id', 'isbn', 'ratings'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m reader \u001b[38;5;241m=\u001b[39m Reader(rating_scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load the dataset from the DataFrame using the appropriate columns\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m data \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mload_from_df(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misbn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratings\u001b[39m\u001b[38;5;124m'\u001b[39m]], reader)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Split the data into training and test sets\u001b[39;00m\n\u001b[0;32m     18\u001b[0m trainset, testset \u001b[38;5;241m=\u001b[39m train_test_split(data, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3766\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3767\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3769\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5879\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5881\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5938\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5936\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[0;32m   5937\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 5938\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5940\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   5941\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['user_id', 'isbn', 'ratings'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n",
    "\n",
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Read the dataset from 'ratings.csv' into a DataFrame\n",
    "df = pd.read_csv('ratings.csv')\n",
    "\n",
    "# Define the Reader object\n",
    "reader = Reader(rating_scale=(0, 10))\n",
    "\n",
    "# Load the dataset from the DataFrame using the appropriate columns\n",
    "data = Dataset.load_from_df(df[['user_id', 'isbn', 'ratings']], reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Initialize KNNBasic with appropriate parameters\n",
    "sim_options = {'name': 'cosine', 'user_based': True}  # Set user-based similarity\n",
    "algo = KNNBasic(k=40, sim_options=sim_options)\n",
    "\n",
    "# Fit the algorithm to the training data\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))\n",
    "print(\"MAE:\", accuracy.mae(predictions))\n",
    "\n",
    "# Generate recommendations for user 'user1'\n",
    "user_id = 'user1'\n",
    "items_to_recommend = []\n",
    "already_rated = set([item[0] for item in trainset.ur[trainset.to_inner_uid(user_id)]])\n",
    "for item_id in trainset.all_items():\n",
    "    if item_id not in already_rated:\n",
    "        items_to_recommend.append((item_id, algo.predict(user_id, item_id).est))\n",
    "\n",
    "# Sort recommendations by predicted rating\n",
    "sorted_recommendations = sorted(items_to_recommend, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top Recommendations:\")\n",
    "for item_id, rating in sorted_recommendations[:10]:\n",
    "    print(f\"Item ID: {item_id}, Predicted Rating: {rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ef017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-surprise in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['isbn', 'ratings', 'user_id']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Read the dataset from 'ratings.csv' into a DataFrame\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Adjust the column names as per your dataset\u001b[39;00m\n\u001b[0;32m     10\u001b[0m column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124misbn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratings\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratings.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, usecols\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Define the Reader object\u001b[39;00m\n\u001b[0;32m     14\u001b[0m reader \u001b[38;5;241m=\u001b[39m Reader(rating_scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:140\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(usecols)\u001b[38;5;241m.\u001b[39missubset(\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names\n\u001b[0;32m    139\u001b[0m ):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_usecols_names(usecols, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(usecols):  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:959\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[1;34m(self, usecols, names)\u001b[0m\n\u001b[0;32m    957\u001b[0m missing \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    961\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[1;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['isbn', 'ratings', 'user_id']"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n",
    "\n",
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Read the dataset from 'ratings.csv' into a DataFrame\n",
    "# Adjust the column names as per your dataset\n",
    "column_names = ['user_id', 'isbn', 'ratings']\n",
    "df = pd.read_csv('ratings.csv', usecols=column_names)\n",
    "\n",
    "# Define the Reader object\n",
    "reader = Reader(rating_scale=(0, 10))\n",
    "\n",
    "# Load the dataset from the DataFrame using the appropriate columns\n",
    "data = Dataset.load_from_df(df, reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Initialize KNNBasic with appropriate parameters\n",
    "sim_options = {'name': 'cosine', 'user_based': True}  # Set user-based similarity\n",
    "algo = KNNBasic(k=40, sim_options=sim_options)\n",
    "\n",
    "# Fit the algorithm to the training data\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))\n",
    "print(\"MAE:\", accuracy.mae(predictions))\n",
    "\n",
    "# Generate recommendations for user 'user1'\n",
    "user_id = 'user1'\n",
    "items_to_recommend = []\n",
    "already_rated = set([item[0] for item in trainset.ur[trainset.to_inner_uid(user_id)]])\n",
    "for item_id in trainset.all_items():\n",
    "    if item_id not in already_rated:\n",
    "        items_to_recommend.append((item_id, algo.predict(user_id, item_id).est))\n",
    "\n",
    "# Sort recommendations by predicted rating\n",
    "sorted_recommendations = sorted(items_to_recommend, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top Recommendations:\")\n",
    "for item_id, rating in sorted_recommendations[:10]:\n",
    "    print(f\"Item ID: {item_id}, Predicted Rating: {rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef0b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-surprise in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['book-author', 'image-url_l', 'book_title', 'year_of_publication', 'Image-url-s', 'image_url_m', 'isbn', 'ratings']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Read the dataset from 'ratings.csv' into a DataFrame\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Adjust the column names as per your dataset\u001b[39;00m\n\u001b[0;32m     10\u001b[0m column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misbn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_title\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook-author\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear_of_publication\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage-url-s\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_url_m\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage-url_l\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratings\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratings.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, usecols\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Define the Reader object\u001b[39;00m\n\u001b[0;32m     14\u001b[0m reader \u001b[38;5;241m=\u001b[39m Reader(rating_scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:140\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(usecols)\u001b[38;5;241m.\u001b[39missubset(\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names\n\u001b[0;32m    139\u001b[0m ):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_usecols_names(usecols, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(usecols):  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:959\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[1;34m(self, usecols, names)\u001b[0m\n\u001b[0;32m    957\u001b[0m missing \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    961\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[1;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['book-author', 'image-url_l', 'book_title', 'year_of_publication', 'Image-url-s', 'image_url_m', 'isbn', 'ratings']"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n",
    "\n",
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Read the dataset from 'ratings.csv' into a DataFrame\n",
    "# Adjust the column names as per your dataset\n",
    "column_names = ['isbn', 'book_title', 'book-author', 'year_of_publication' , 'Image-url-s', 'image_url_m' , 'image-url_l', 'ratings']\n",
    "df = pd.read_csv('ratings.csv', usecols=column_names)\n",
    "\n",
    "# Define the Reader object\n",
    "reader = Reader(rating_scale=(0, 10))\n",
    "\n",
    "# Load the dataset from the DataFrame using the appropriate columns\n",
    "data = Dataset.load_from_df(df, reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Initialize KNNBasic with appropriate parameters\n",
    "sim_options = {'name': 'cosine', 'user_based': True}  # Set user-based similarity\n",
    "algo = KNNBasic(k=40, sim_options=sim_options)\n",
    "\n",
    "# Fit the algorithm to the training data\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))\n",
    "print(\"MAE:\", accuracy.mae(predictions))\n",
    "\n",
    "# Generate recommendations for user 'user1'\n",
    "user_id = 'user1'\n",
    "items_to_recommend = []\n",
    "already_rated = set([item[0] for item in trainset.ur[trainset.to_inner_uid(user_id)]])\n",
    "for item_id in trainset.all_items():\n",
    "    if item_id not in already_rated:\n",
    "        items_to_recommend.append((item_id, algo.predict(user_id, item_id).est))\n",
    "\n",
    "# Sort recommendations by predicted rating\n",
    "sorted_recommendations = sorted(items_to_recommend, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top Recommendations:\")\n",
    "for item_id, rating in sorted_recommendations[:10]:\n",
    "    print(f\"Item ID: {item_id}, Predicted Rating: {rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157feb81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-surprise in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['book-author', 'image-url_l', 'book_title', 'year_of_publication', 'Image-url-s', 'image_url_m', 'isbn', 'ratings']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Read the dataset from 'ratings.csv' into a DataFrame\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Adjust the column names as per your dataset\u001b[39;00m\n\u001b[0;32m     10\u001b[0m column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misbn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_title\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook-author\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear_of_publication\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage-url-s\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_url_m\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage-url_l\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratings\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratings.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, usecols\u001b[38;5;241m=\u001b[39mcolumn_names)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Define the Reader object\u001b[39;00m\n\u001b[0;32m     14\u001b[0m reader \u001b[38;5;241m=\u001b[39m Reader(rating_scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:140\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39musecols_dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mset\u001b[39m(usecols)\u001b[38;5;241m.\u001b[39missubset(\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names\n\u001b[0;32m    139\u001b[0m ):\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_usecols_names(usecols, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_names)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(usecols):  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:959\u001b[0m, in \u001b[0;36mParserBase._validate_usecols_names\u001b[1;34m(self, usecols, names)\u001b[0m\n\u001b[0;32m    957\u001b[0m missing \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m usecols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m names]\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    960\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsecols do not match columns, columns expected but not found: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    961\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m usecols\n",
      "\u001b[1;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['book-author', 'image-url_l', 'book_title', 'year_of_publication', 'Image-url-s', 'image_url_m', 'isbn', 'ratings']"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n",
    "\n",
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Read the dataset from 'ratings.csv' into a DataFrame\n",
    "# Adjust the column names as per your dataset\n",
    "column_names = ['isbn', 'book_title', 'book-author', 'year_of_publication' , 'Image-url-s', 'image_url_m' , 'image-url_l', 'ratings']\n",
    "df = pd.read_csv('ratings.csv', usecols=column_names)\n",
    "\n",
    "# Define the Reader object\n",
    "reader = Reader(rating_scale=(0, 10))\n",
    "\n",
    "# Load the dataset from the DataFrame using the appropriate columns\n",
    "data = Dataset.load_from_df(df, reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Initialize KNNBasic with appropriate parameters\n",
    "sim_options = {'name': 'cosine', 'user_based': True}  # Set user-based similarity\n",
    "algo = KNNBasic(k=40, sim_options=sim_options)\n",
    "\n",
    "# Fit the algorithm to the training data\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))\n",
    "print(\"MAE:\", accuracy.mae(predictions))\n",
    "\n",
    "# Generate recommendations for user 'user1'\n",
    "user_id = 'user1'\n",
    "items_to_recommend = []\n",
    "already_rated = set([item[0] for item in trainset.ur[trainset.to_inner_uid(user_id)]])\n",
    "for item_id in trainset.all_items():\n",
    "    if item_id not in already_rated:\n",
    "        items_to_recommend.append((item_id, algo.predict(user_id, item_id).est))\n",
    "\n",
    "# Sort recommendations by predicted rating\n",
    "sorted_recommendations = sorted(items_to_recommend, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top Recommendations:\")\n",
    "for item_id, rating in sorted_recommendations[:10]:\n",
    "    print(f\"Item ID: {item_id}, Predicted Rating: {rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede4711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-surprise in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_1404\\3176217601.py:11: DtypeWarning: Columns (0,1,2,4,5,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('ratings.csv', usecols=column_names)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m reader \u001b[38;5;241m=\u001b[39m Reader(rating_scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Load the dataset from the DataFrame using the appropriate columns\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m data \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mload_from_df(df, reader)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Split the data into training and test sets\u001b[39;00m\n\u001b[0;32m     20\u001b[0m trainset, testset \u001b[38;5;241m=\u001b[39m train_test_split(data, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\surprise\\dataset.py:167\u001b[0m, in \u001b[0;36mDataset.load_from_df\u001b[1;34m(cls, df, reader)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_df\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, reader):\n\u001b[0;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a dataset from a pandas dataframe.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    Use this if you want to use a custom dataset that is stored in a pandas\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m            specified.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatasetAutoFolds(reader\u001b[38;5;241m=\u001b[39mreader, df\u001b[38;5;241m=\u001b[39mdf)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\surprise\\dataset.py:262\u001b[0m, in \u001b[0;36mDatasetAutoFolds.__init__\u001b[1;34m(self, ratings_file, reader, df)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m df\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_ratings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    263\u001b[0m         (uid, iid, \u001b[38;5;28mfloat\u001b[39m(r), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (uid, iid, r) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mitertuples(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    265\u001b[0m     ]\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust specify ratings file or dataframe.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\surprise\\dataset.py:264\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m df\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_ratings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    263\u001b[0m         (uid, iid, \u001b[38;5;28mfloat\u001b[39m(r), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 264\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (uid, iid, r) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mitertuples(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    265\u001b[0m     ]\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust specify ratings file or dataframe.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n",
    "\n",
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Read the dataset from 'ratings.csv' into a DataFrame\n",
    "# Adjust the column names as per your dataset\n",
    "column_names = ['isbn', 'book_title', 'book-author', 'year_of_publication', 'publisher', 'Image-url-s', 'image_url_m', 'image-url_l', 'ratings']\n",
    "df = pd.read_csv('ratings.csv', usecols=column_names)\n",
    "\n",
    "# Define the Reader object\n",
    "reader = Reader(rating_scale=(0, 10))\n",
    "\n",
    "# Load the dataset from the DataFrame using the appropriate columns\n",
    "data = Dataset.load_from_df(df, reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Initialize KNNBasic with appropriate parameters\n",
    "sim_options = {'name': 'cosine', 'user_based': True}  # Set user-based similarity\n",
    "algo = KNNBasic(k=40, sim_options=sim_options)\n",
    "\n",
    "# Fit the algorithm to the training data\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))\n",
    "print(\"MAE:\", accuracy.mae(predictions))\n",
    "\n",
    "# Generate recommendations for user 'user1'\n",
    "user_id = 'user1'\n",
    "items_to_recommend = []\n",
    "already_rated = set([item[0] for item in trainset.ur[trainset.to_inner_uid(user_id)]])\n",
    "for item_id in trainset.all_items():\n",
    "    if item_id not in already_rated:\n",
    "        items_to_recommend.append((item_id, algo.predict(user_id, item_id).est))\n",
    "\n",
    "# Sort recommendations by predicted rating\n",
    "sorted_recommendations = sorted(items_to_recommend, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top Recommendations:\")\n",
    "for item_id, rating in sorted_recommendations[:10]:\n",
    "    print(f\"Item ID: {item_id}, Predicted Rating: {rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734bdd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-surprise in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Integer column has NA values in column 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Read the dataset from 'ratings.csv' into a DataFrame\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Adjust the column names as per your dataset\u001b[39;00m\n\u001b[0;32m     10\u001b[0m column_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misbn\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_title\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook-author\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear_of_publication\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage-url-s\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_url_m\u001b[39m\u001b[38;5;124m'\u001b[39m , \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage-url_l\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratings\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratings.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, usecols\u001b[38;5;241m=\u001b[39mcolumn_names, dtype\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misbn\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook_title\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbook-author\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear_of_publication\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mint\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImage-url-s\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_url_m\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage-url_l\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mratings\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mfloat\u001b[39m})\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Define the Reader object\u001b[39;00m\n\u001b[0;32m     14\u001b[0m reader \u001b[38;5;241m=\u001b[39m Reader(rating_scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     (\n\u001b[0;32m   1701\u001b[0m         index,\n\u001b[0;32m   1702\u001b[0m         columns,\n\u001b[0;32m   1703\u001b[0m         col_dict,\n\u001b[1;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m         nrows\n\u001b[0;32m   1706\u001b[0m     )\n\u001b[0;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1036\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1075\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1196\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Integer column has NA values in column 3"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n",
    "\n",
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Read the dataset from 'ratings.csv' into a DataFrame\n",
    "# Adjust the column names as per your dataset\n",
    "column_names = ['isbn', 'book_title', 'book-author', 'year_of_publication' , 'Image-url-s', 'image_url_m' , 'image-url_l', 'ratings']\n",
    "df = pd.read_csv('ratings.csv', usecols=column_names, dtype={'isbn': str, 'book_title': str, 'book-author': str, 'year_of_publication': int, 'Image-url-s': str, 'image_url_m': str, 'image-url_l': str, 'ratings': float})\n",
    "\n",
    "# Define the Reader object\n",
    "reader = Reader(rating_scale=(0, 10))\n",
    "\n",
    "# Load the dataset from the DataFrame using the appropriate columns\n",
    "data = Dataset.load_from_df(df, reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Initialize KNNBasic with appropriate parameters\n",
    "sim_options = {'name': 'cosine', 'user_based': True}  # Set user-based similarity\n",
    "algo = KNNBasic(k=40, sim_options=sim_options)\n",
    "\n",
    "# Fit the algorithm to the training data\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))\n",
    "print(\"MAE:\", accuracy.mae(predictions))\n",
    "\n",
    "# Generate recommendations for user 'user1'\n",
    "user_id = 'user1'\n",
    "items_to_recommend = []\n",
    "already_rated = set([item[0] for item in trainset.ur[trainset.to_inner_uid(user_id)]])\n",
    "for item_id in trainset.all_items():\n",
    "    if item_id not in already_rated:\n",
    "        items_to_recommend.append((item_id, algo.predict(user_id, item_id).est))\n",
    "\n",
    "# Sort recommendations by predicted rating\n",
    "sorted_recommendations = sorted(items_to_recommend, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top Recommendations:\")\n",
    "for item_id, rating in sorted_recommendations[:10]:\n",
    "    print(f\"Item ID: {item_id}, Predicted Rating: {rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6415e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-surprise in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.11.1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m reader \u001b[38;5;241m=\u001b[39m Reader(rating_scale\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Load the dataset from the DataFrame using the appropriate columns\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m data \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mload_from_df(df, reader)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Split the data into training and test sets\u001b[39;00m\n\u001b[0;32m     22\u001b[0m trainset, testset \u001b[38;5;241m=\u001b[39m train_test_split(data, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\surprise\\dataset.py:167\u001b[0m, in \u001b[0;36mDataset.load_from_df\u001b[1;34m(cls, df, reader)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_df\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, reader):\n\u001b[0;32m    152\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a dataset from a pandas dataframe.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    Use this if you want to use a custom dataset that is stored in a pandas\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m            specified.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DatasetAutoFolds(reader\u001b[38;5;241m=\u001b[39mreader, df\u001b[38;5;241m=\u001b[39mdf)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\surprise\\dataset.py:262\u001b[0m, in \u001b[0;36mDatasetAutoFolds.__init__\u001b[1;34m(self, ratings_file, reader, df)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m df\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_ratings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    263\u001b[0m         (uid, iid, \u001b[38;5;28mfloat\u001b[39m(r), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    264\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (uid, iid, r) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mitertuples(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    265\u001b[0m     ]\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust specify ratings file or dataframe.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\surprise\\dataset.py:264\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m df\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_ratings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    263\u001b[0m         (uid, iid, \u001b[38;5;28mfloat\u001b[39m(r), \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 264\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (uid, iid, r) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mitertuples(index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    265\u001b[0m     ]\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust specify ratings file or dataframe.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "!pip install scikit-surprise\n",
    "\n",
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Read the dataset from 'ratings.csv' into a DataFrame\n",
    "# Adjust the column names as per your dataset\n",
    "column_names = ['user_id', 'isbn', 'book_title', 'book-author', 'year_of_publication' , 'Image-url-s', 'image_url_m' , 'image-url_l', 'ratings']\n",
    "df = pd.read_csv('ratings.csv', usecols=column_names, dtype={'user_id':float,'isbn': str, 'book_title': str, 'book-author': str, 'year_of_publication': float, 'Image-url-s': str, 'image_url_m': str, 'image-url_l': str, 'ratings': float}, na_values=['NA', 'N/A', 'missing'])\n",
    "\n",
    "df.dropna(subset=['year_of_publication'], inplace=True)\n",
    "\n",
    "# Define the Reader object\n",
    "reader = Reader(rating_scale=(0, 10))\n",
    "\n",
    "# Load the dataset from the DataFrame using the appropriate columns\n",
    "data = Dataset.load_from_df(df, reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Initialize KNNBasic with appropriate parameters\n",
    "sim_options = {'name': 'cosine', 'user_based': True}  # Set user-based similarity\n",
    "algo = KNNBasic(k=40, sim_options=sim_options)\n",
    "\n",
    "# Fit the algorithm to the training data\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))\n",
    "print(\"MAE:\", accuracy.mae(predictions))\n",
    "\n",
    "# Generate recommendations for user 'user1'\n",
    "user_id = 'user1'\n",
    "items_to_recommend = []\n",
    "already_rated = set([item[0] for item in trainset.ur[trainset.to_inner_uid(user_id)]])\n",
    "for item_id in trainset.all_items():\n",
    "    if item_id not in already_rated:\n",
    "        items_to_recommend.append((item_id, algo.predict(user_id, item_id).est))\n",
    "\n",
    "# Sort recommendations by predicted rating\n",
    "sorted_recommendations = sorted(items_to_recommend, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top Recommendations:\")\n",
    "for item_id, rating in sorted_recommendations[:10]:\n",
    "    print(f\"Item ID: {item_id}, Predicted Rating: {rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b09a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 2.9250\n",
      "RMSE: 2.925004352105671\n",
      "MAE:  2.5488\n",
      "MAE: 2.548840720057558\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "User user1 is not part of the trainset.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\surprise\\trainset.py:110\u001b[0m, in \u001b[0;36mTrainset.to_inner_uid\u001b[1;34m(self, ruid)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw2inner_id_users[ruid]\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'user1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m user_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     39\u001b[0m items_to_recommend \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 40\u001b[0m already_rated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([item[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m trainset\u001b[38;5;241m.\u001b[39mur[trainset\u001b[38;5;241m.\u001b[39mto_inner_uid(user_id)]])\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item_id \u001b[38;5;129;01min\u001b[39;00m trainset\u001b[38;5;241m.\u001b[39mall_items():\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m already_rated:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\surprise\\trainset.py:112\u001b[0m, in \u001b[0;36mTrainset.to_inner_uid\u001b[1;34m(self, ruid)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raw2inner_id_users[ruid]\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(ruid) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not part of the trainset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: User user1 is not part of the trainset."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Read the dataset from 'ratings.csv' into a DataFrame\n",
    "# Adjust the column names as per your dataset\n",
    "column_names = ['user_id', 'isbn', 'ratings']\n",
    "df = pd.read_csv('ratings.csv', usecols=column_names, dtype={'user_id': str, 'isbn': str, 'ratings': float}, na_values=['NA', 'N/A', 'missing'])\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(subset=['user_id', 'isbn', 'ratings'], inplace=True)\n",
    "\n",
    "# Define the Reader object\n",
    "reader = Reader(rating_scale=(0, 10))\n",
    "\n",
    "# Load the dataset from the DataFrame using the appropriate columns\n",
    "data = Dataset.load_from_df(df[['user_id', 'isbn', 'ratings']], reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Initialize KNNBasic with appropriate parameters\n",
    "sim_options = {'name': 'cosine', 'user_based': True}  # Set user-based similarity\n",
    "algo = KNNBasic(k=40, sim_options=sim_options)\n",
    "\n",
    "# Fit the algorithm to the training data\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))\n",
    "print(\"MAE:\", accuracy.mae(predictions))\n",
    "\n",
    "# Generate recommendations for user 'user1'\n",
    "user_id = 'user1'\n",
    "items_to_recommend = []\n",
    "already_rated = set([item[0] for item in trainset.ur[trainset.to_inner_uid(user_id)]])\n",
    "for item_id in trainset.all_items():\n",
    "    if item_id not in already_rated:\n",
    "        items_to_recommend.append((item_id, algo.predict(user_id, item_id).est))\n",
    "\n",
    "# Sort recommendations by predicted rating\n",
    "sorted_recommendations = sorted(items_to_recommend, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top Recommendations:\")\n",
    "for item_id, rating in sorted_recommendations[:10]:\n",
    "    print(f\"Item ID: {item_id}, Predicted Rating: {rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f1d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 2.8927\n",
      "RMSE: 2.892652908822402\n",
      "MAE:  2.5098\n",
      "MAE: 2.509774606062856\n",
      "Top Recommendations for user1\n",
      "Item ID: 0, Predicted Rating: 5.06\n",
      "Item ID: 1, Predicted Rating: 5.06\n",
      "Item ID: 2, Predicted Rating: 5.06\n",
      "Item ID: 3, Predicted Rating: 5.06\n",
      "Item ID: 4, Predicted Rating: 5.06\n",
      "Item ID: 5, Predicted Rating: 5.06\n",
      "Item ID: 6, Predicted Rating: 5.06\n",
      "Item ID: 7, Predicted Rating: 5.06\n",
      "Item ID: 8, Predicted Rating: 5.06\n",
      "Item ID: 9, Predicted Rating: 5.06\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "\n",
    "# Read the dataset from 'ratings.csv' into a DataFrame\n",
    "# Adjust the column names as per your dataset\n",
    "column_names = ['user_id', 'isbn', 'ratings']\n",
    "df = pd.read_csv('ratings.csv', usecols=column_names, dtype={'user_id': str, 'isbn': str, 'ratings': float}, na_values=['NA', 'N/A', 'missing'])\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(subset=['user_id', 'isbn', 'ratings'], inplace=True)\n",
    "\n",
    "# Define the Reader object\n",
    "reader = Reader(rating_scale=(0, 10))\n",
    "\n",
    "# Load the dataset from the DataFrame using the appropriate columns\n",
    "data = Dataset.load_from_df(df[['user_id', 'isbn', 'ratings']], reader)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "trainset, testset = train_test_split(data, test_size=0.25)\n",
    "\n",
    "# Initialize KNNBasic with appropriate parameters\n",
    "sim_options = {'name': 'cosine', 'user_based': True}  # Set user-based similarity\n",
    "algo = KNNBasic(k=40, sim_options=sim_options)\n",
    "\n",
    "# Fit the algorithm to the training data\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Evaluate accuracy\n",
    "print(\"RMSE:\", accuracy.rmse(predictions))\n",
    "print(\"MAE:\", accuracy.mae(predictions))\n",
    "\n",
    "# Generate recommendations for user 'user1'\n",
    "user_id = 'user1'\n",
    "\n",
    "try:\n",
    "    # Get the inner user ID corresponding to 'user1'\n",
    "    inner_user_id = trainset.to_inner_uid(user_id)\n",
    "\n",
    "    # Get the items already rated by 'user1'\n",
    "    already_rated = set([item[0] for item in trainset.ur[inner_user_id]])\n",
    "\n",
    "    # Generate recommendations for items not yet rated by 'user1'\n",
    "    items_to_recommend = []\n",
    "    for item_id in trainset.all_items():\n",
    "        if item_id not in already_rated:\n",
    "            items_to_recommend.append((item_id, algo.predict(user_id, item_id).est))\n",
    "\n",
    "    # Sort recommendations by predicted rating\n",
    "    sorted_recommendations = sorted(items_to_recommend, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(\"Top Recommendations for\", user_id)\n",
    "    for item_id, rating in sorted_recommendations[:10]:\n",
    "        print(f\"Item ID: {item_id}, Predicted Rating: {rating:.2f}\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(e)  # Handle the case where 'user1' is not part of the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842a2977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
